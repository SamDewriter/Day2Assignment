{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYRi1ziGDf05"
   },
   "source": [
    "<h1> PANDAS </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXNgbNcoDf07"
   },
   "source": [
    "Pandas is a python package for data query and analysis. It is well known for its versatility in reading numerous types of data file.\n",
    "It also allows for simple data munging and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm5u7WN1Df08"
   },
   "source": [
    "<h1> MATPLOTLIB </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRvgh6zZDf08"
   },
   "source": [
    "This is exclusively a visualization library in python. It is employed for visual involving 2D, 3D and animation. It also serves as dependency for other numerous libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vpW18KZDf08"
   },
   "source": [
    "<h1> SCIKIT-LEARN </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU7hOKNqDf09"
   },
   "source": [
    "An almost-complete library for data analysis and modelling is the <b>sklearn</b> library. It contains various statistical models and few neural network model one might require in any analytic problem. While thre are many modelling libraries out there, <b>sklearn</b> on it's own houses numerous modelling techniques as modules and functions all which come together to make up the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2n8e5BTdDf09"
   },
   "source": [
    "<h1> MODELLING AND DEPLOYMENT </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGNnsgaSDf0-"
   },
   "source": [
    "Unlike data wrangling, this is widely the interesting part in general data analysis. The step here are as follows:\n",
    "\n",
    "$\\bullet$ determine the set of algorithms to try on the data (classification, regression, neural-net etc).\n",
    "\n",
    "$\\bullet$ model design - data splitting.\n",
    "\n",
    "$\\bullet$ model building\n",
    "\n",
    "$\\bullet$ evaluation (metrics)\n",
    "\n",
    "$\\bullet$ model review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrKjwZQFDf0_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7yoPbvoDf1A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJs1KGfNDf1A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMqIuxbxDf1A"
   },
   "source": [
    "<h2><a href=\"https://github.com/lazuxd/simple-imdb-sentiment-analysis/blob/master/sentiment-analysis.ipynb\"> Notebook reference </a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWWkJFklDf1B"
   },
   "source": [
    "# Building a Sentiment Classifier using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSP0Q-lUDf1B"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/lazuxd/simple-imdb-sentiment-analysis/master/smiley.jpg\"/></center>\n",
    "<center><i>Image by AbsolutVision @ <a href=\"https://pixabay.com/ro/photos/smiley-emoticon-furie-sup%C4%83rat-2979107/\">pixabay.com</a></i></center>\n",
    "\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;**Sentiment analysis**, an important area in Natural Language Processing, is the process of automatically detecting affective states of text. Sentiment analysis is widely applied to voice-of-customer materials such as product reviews in online shopping websites like Amazon, movie reviews or social media. It can be just a basic task of classifying the polarity of a text as being positive/negative or it can go beyond polarity, looking at emotional states such as \"happy\", \"angry\", etc.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Here we will build a classifier that is able to distinguish movie reviews as being either positive or negative. For that, we will use [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)<sup>(2)</sup> of IMDB movie reviews.\n",
    "This dataset contains 50,000 movie reviews divided evenly into 25k train and 25k test. The labels are balanced between the two classes (positive and negative). Reviews with a score <= 4 out of 10 are labeled negative and those with score >= 7 out of 10 are labeled positive. Neutral reviews are not included in the labeled data. This dataset also contains unlabeled reviews for unsupervised learning; we will not use them here. There are no more than 30 reviews for a particular movie because the ratings of the same movie tend to be correlated. All reviews for a given movie are either in train or test set but not in both, in order to avoid test accuracy gain by memorizing movie-specific terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8deikMzDf1C"
   },
   "source": [
    "## Downloading & extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oGbIpj6Df1C",
    "outputId": "9d70517f-387a-4155-d227-ab6bd3e9fe5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-20 13:42:11--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M   770KB/s    in 2m 43s  \n",
      "\n",
      "2021-06-20 13:44:54 (505 KB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPONpw8fDf1E"
   },
   "outputs": [],
   "source": [
    "#!tar -xzf \"aclImdb_v1.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z87ZVw5Df1E"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chTXW5hGDf1E"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;After the dataset has been downloaded and extracted from archive we have to transform it into a more suitable form for feeding it into a machine learning model for training. We will start by combining all review data into 2 pandas Data Frames representing the train and test datasets, and then saving them as csv files: *imdb_train.csv* and *imdb_test.csv*.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The Data Frames will have the following form:  \n",
    "\n",
    "|text       |label      |\n",
    "|:---------:|:---------:|\n",
    "|review1    |0          |\n",
    "|review2    |1          |\n",
    "|review3    |1          |\n",
    "|.......    |...        |\n",
    "|reviewN    |0          |  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "- review1, review2, ... = the actual text of movie review  \n",
    "- 0 = negative review  \n",
    "- 1 = positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF630d-aDf1F"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;But machine learnng algorithms work only with numerical values. We can't just input the text itself into a machine learning model and have it learn from that. We have to, somehow, represent the text by numbers or vectors of numbers. One way of doing this is by using the **Bag-of-words** model<sup>(3)</sup>, in which a piece of text(often called a **document**) is represented by a vector of the counts of words from a vocabulary in that document. This model doesn't take into account grammar rules or word ordering; all it considers is the frequency of words. If we use the counts of each word independently we name this representation a **unigram**. In general, in a **n-gram** we take into account the counts of each combination of n words from the vocabulary that appears in a given document.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For example, consider these two documents:  \n",
    "<br>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d1: \"I am learning\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></center></div>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d2: \"Machine learning is cool\"</b></center></div>  \n",
    "<br>\n",
    "The vocabulary of all words encountered in these two sentences is: \n",
    "\n",
    "<br/>  \n",
    "<div style=\"font-family: monospace;\"><center><b>v: [ I, am, learning, machine, is, cool ]</b></center></div>   \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The unigram representations of d1 and d2:  \n",
    "<br>  \n",
    "\n",
    "|unigram(d1)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |1       |1       |1       |0       |0       |0       |  \n",
    "\n",
    "|unigram(d2)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |0       |0       |1       |1       |1       |1       |\n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;And, the bigrams of d1 and d2 are:\n",
    "  \n",
    "|bigram(d1) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |1       |0         |...|0         |0               |...|0      |0        |  \n",
    "\n",
    "|bigram(d2) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |0       |0         |...|0         |1               |...|0      |0        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCiHAtSSDf1G"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Often, we can achieve slightly better results if instead of counts of words we use something called **term frequency times inverse document frequency** (or **tf-idf**). Maybe it sounds complicated, but it is not. Bear with me, I will explain this. The intuition behind this is the following. So, what's the problem of using just the frequency of terms inside a document? Although some terms may have a high frequency inside documents they may not be so relevant for describing a given document in which they appear. That's because those terms may also have a high frequency across the collection of all documents. For example, a collection of movie reviews may have terms specific to movies/cinematography that are present in almost all documents(they have a high **document frequency**). So, when we encounter those terms in a document this doesn't tell much about whether it is a positive or negative review. We need a way of relating **term frequency** (how frequent a term is inside a document) to **document frequency** (how frequent a term is across the whole collection of documents). That is:  \n",
    "  \n",
    "$$\\begin{align}\\frac{\\text{term frequency}}{\\text{document frequency}} &= \\text{term frequency} \\cdot \\frac{1}{\\text{document frequency}} \\\\ &= \\text{term frequency} \\cdot \\text{inverse document frequency} \\\\ &= \\text{tf} \\cdot \\text{idf}\\end{align}$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, there are more ways used to describe both term frequency and inverse document frequency. But the most common way is by putting them on a logarithmic scale:  \n",
    "  \n",
    "$$tf(t, d) = log(1+f_{t,d})$$  \n",
    "$$idf(t) = log(\\frac{1+N}{1+n_t})$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "$$\\begin{align}f_{t,d} &= \\text{count of term } \\textbf{t} \\text{ in document } \\textbf{d} \\\\  \n",
    "N &= \\text{total number of documents} \\\\  \n",
    "n_t &= \\text{number of documents that contain term } \\textbf{t}\\end{align}$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We added 1 in the first logarithm to avoid getting $-\\infty$ when $f_{t,d}$ is 0. In the second logarithm we added one fake document to avoid division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onj_COYzDf1H"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Before we transform our data into vectors of counts or tf-idf values we should remove English **stopwords**<sup>(6)(7)</sup>. Stopwords are words that are very common in a language and are usually removed in the preprocessing stage of natural text-related tasks like sentiment analysis or search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bluQ2jTmDf1I"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Note that we should construct our vocabulary only based on the training set. When we will process the test data in order to make predictions we should use only the vocabulary constructed in the training phase, the rest of the words will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvBWdNOYDf1J"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, let's create the data frames and save them as csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMWzlLoWDf1K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXIF2beADf1K"
   },
   "outputs": [],
   "source": [
    "def create_data_frame(folder: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    folder - the root folder of train or test dataset\n",
    "    Returns: a DataFrame with the combined data from the input folder\n",
    "    '''\n",
    "    pos_folder = f'{folder}/pos' # positive reviews\n",
    "    neg_folder = f'{folder}/neg' # negative reviews\n",
    "    \n",
    "    def get_files(fld: str) -> list:\n",
    "        '''\n",
    "        fld - positive or negative reviews folder\n",
    "        Returns: a list with all files in input folder\n",
    "        '''\n",
    "        return [join(fld, f) for f in listdir(fld) if isfile(join(fld, f))]\n",
    "    \n",
    "    def append_files_data(data_list: list, files: list, label: int) -> None:\n",
    "        '''\n",
    "        Appends to 'data_list' tuples of form (file content, label)\n",
    "        for each file in 'files' input list\n",
    "        '''\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                text = f.read()\n",
    "                data_list.append((text, label))\n",
    "    \n",
    "    pos_files = get_files(pos_folder)\n",
    "    neg_files = get_files(neg_folder)\n",
    "    \n",
    "    data_list = []\n",
    "    append_files_data(data_list, pos_files, 1)\n",
    "    append_files_data(data_list, neg_files, 0)\n",
    "    shuffle(data_list)\n",
    "    \n",
    "    text, label = tuple(zip(*data_list))\n",
    "    # replacing line breaks with spaces\n",
    "    text = list(map(lambda txt: re.sub('(<br\\s*/?>)+', ' ', txt), text))\n",
    "    \n",
    "    return pd.DataFrame({'text': text, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZpb5gm7Df1K",
    "outputId": "f5e1410c-1657-4522-ecc9-a71832449f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘csv’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "imdb_train = create_data_frame('aclImdb/train')\n",
    "imdb_test = create_data_frame('aclImdb/test')\n",
    "\n",
    "!mkdir 'csv'\n",
    "imdb_train.to_csv('csv/imdb_train.csv', index=False)\n",
    "imdb_test.to_csv('csv/imdb_test.csv', index=False)\n",
    "\n",
    "# imdb_train = pd.read_csv('csv/imdb_train.csv')\n",
    "# imdb_test = pd.read_csv('csv/imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm9zR5mvDf1L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLGlqf6fDf1L"
   },
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2q7_0tYDf1M"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Fortunately, for the text vectorization part all the hard work is already done in the Scikit-Learn classes `CountVectorizer`<sup>(8)</sup> and `TfidfTransformer`<sup>(5)</sup>. We will use these classes to transform our csv files into unigram and bigram matrices(using both counts and tf-idf values). (It turns out that if we only use a n-gram for a large n we don't get a good accuracy, we usually use all n-grams up to some n. So, when we say here bigrams we actually refer to uni+bigrams and when we say unigrams it's just unigrams.) Each row in those matrices will represent a document (review) in our dataset, and each column will represent values associated with each word in the vocabulary (in the case of unigrams) or values associated with each combination of maximum 2 words in the vocabulary (bigrams).  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`CountVectorizer` has a parameter `ngram_range` which expects a tuple of size 2 that controls what n-grams to include. After we constructed a `CountVectorizer` object we should call `.fit()` method with the actual text as a parameter, in order for it to learn the required statistics of our collection of documents. Then, by calling `.transform()` method with our collection of documents it returns the matrix for the n-gram range specified. As the class name suggests, this matrix will contain just the counts. To obtain the tf-idf values, the class `TfidfTransformer` should be used. It has the `.fit()` and `.transform()` methods that are used in a similar way with those of `CountVectorizer`, but they take as input the counts matrix obtained in the previous step and `.transform()` will return a matrix with tf-idf values. We should use `.fit()` only on training data and then store these objects. When we want to evaluate the test score or whenever we want to make a prediction we should use these objects to transform the data before feeding it into our classifier.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Note that the matrices generated for our train or test data will be huge, and if we store them as normal numpy arrays they will not even fit into RAM. But most of the entries in these matrices will be zero. So, these Scikit-Learn classes are using Scipy sparse matrices<sup>(9)</sup> (`csr_matrix`<sup>(10)</sup> to be more exactly), which store just the non-zero entries and save a LOT of space.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We will use a linear classifier with stochastic gradient descent, `sklearn.linear_model.SGDClassifier`<sup>(11)</sup>, as our model. First we will generate and save our data in 4 forms: unigram and bigram matrix (with both counts and tf-idf values for each). Then we will train and evaluate our model for each these 4 data representations using `SGDClassifier` with the default parameters. After that, we choose the data representation which led to the best score and we will tune the hyper-parameters of our model with this data form using cross-validation in order to obtain the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox19B0hSDf1M"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrZLxYSADf1M"
   },
   "outputs": [],
   "source": [
    "#!mkdir 'data_preprocessors'\n",
    "#!mkdir 'vectorized_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oViWmsfUDf1M"
   },
   "source": [
    "#### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBsQHwRxDf1N",
    "outputId": "a7415030-3b65-4ea6-88d1-61eace11a89c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/unigram_vectorizer.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(imdb_train['text'].values)\n",
    "\n",
    "dump(unigram_vectorizer, 'data_preprocessors/unigram_vectorizer.joblib')\n",
    "\n",
    "# unigram_vectorizer = load('data_preprocessors/unigram_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3Qn3cfeDf1O"
   },
   "outputs": [],
   "source": [
    "X_train_unigram = unigram_vectorizer.transform(imdb_train['text'].values)\n",
    "\n",
    "save_npz('vectorized_data/X_train_unigram.npz', X_train_unigram)\n",
    "\n",
    "# X_train_unigram = load_npz('vectorized_data/X_train_unigram.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGCq5_wODf1O"
   },
   "source": [
    "#### Unigram Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gyloqm9BDf1O",
    "outputId": "9e848b8c-46b1-4f27-b9ee-a7f51239ff9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/unigram_tf_idf_transformer.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)\n",
    "\n",
    "dump(unigram_tf_idf_transformer, 'data_preprocessors/unigram_tf_idf_transformer.joblib')\n",
    "\n",
    "# unigram_tf_idf_transformer = load('data_preprocessors/unigram_tf_idf_transformer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54vFfWOLDf1P"
   },
   "outputs": [],
   "source": [
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_unigram_tf_idf.npz', X_train_unigram_tf_idf)\n",
    "\n",
    "# X_train_unigram_tf_idf = load_npz('vectorized_data/X_train_unigram_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyAQzhRJDf1P"
   },
   "source": [
    "#### Bigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzeADPVCDf1P",
    "outputId": "1e2e9905-63b0-4a23-b391-678a5d3dbdf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/bigram_vectorizer.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(imdb_train['text'].values)\n",
    "\n",
    "dump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')\n",
    "\n",
    "# bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGJajXscDf1Q"
   },
   "outputs": [],
   "source": [
    "X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)\n",
    "\n",
    "# X_train_bigram = load_npz('vectorized_data/X_train_bigram.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbE6DZMjDf1Q"
   },
   "source": [
    "#### Bigram Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPUZxwS1Df1Q",
    "outputId": "2fb3811d-089c-46b1-d0e3-797d1b8bb6c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_preprocessors/bigram_tf_idf_transformer.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "dump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "# bigram_tf_idf_transformer = load('data_preprocessors/bigram_tf_idf_transformer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bF_vxbFDf1R"
   },
   "outputs": [],
   "source": [
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "\n",
    "save_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n",
    "\n",
    "# X_train_bigram_tf_idf = load_npz('vectorized_data/X_train_bigram_tf_idf.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6kPMrMkDf1T"
   },
   "source": [
    "### Choosing data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4KJ3tbgDf1U"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, for each data form we split it into train & validation sets, train a `SGDClassifier` and output the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvuHIKseDf1U"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9D3nA5mDf1V"
   },
   "outputs": [],
   "source": [
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnoEFHjRDf1V"
   },
   "outputs": [],
   "source": [
    "y_train = imdb_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuvM4R9NDf1W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNpQfwLWDf1W",
    "outputId": "b9bb7072-97a2-4309-d087-e34356f74ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts\n",
      "Train score: 0.99 ; Validation score: 0.87\n",
      "\n",
      "Unigram Tf-Idf\n",
      "Train score: 0.95 ; Validation score: 0.89\n",
      "\n",
      "Bigram Counts\n",
      "Train score: 1.0 ; Validation score: 0.88\n",
      "\n",
      "Bigram Tf-Idf\n",
      "Train score: 0.98 ; Validation score: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0OIG9qcDf1W"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;The best data form seems to be **bigram with tf-idf** as it gets the highest validation accuracy: **0.9**; we will use it next for hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3oEgNdJDf1X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HssYic3Df1X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35OTN0jTDf1X"
   },
   "source": [
    "<h1> TUTORIAL </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMtNgZQvDf1X"
   },
   "source": [
    "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
    "\n",
    "\n",
    "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
    "\n",
    "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
    "\n",
    "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
    "\n",
    "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
    "\n",
    "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
    "\n",
    "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
    "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
    "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
    "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
    "\n",
    "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
    "\n",
    "* Evaluate your model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "osEIfW-aDf1Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'created_at', 'source', 'original_text', 'clean_text',\n",
       "       'sentiment', 'polarity', 'subjectivity', 'lang', 'favorite_count',\n",
       "       'retweet_count', 'original_author', 'screen_count', 'followers_count',\n",
       "       'friends_count', 'possibly_sensitive', 'hashtags', 'user_mentions',\n",
       "       'place', 'place_coord_boundaries', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data\\Copy of cleaned_fintech_data.csv')\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N4C0tR76Df1Y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Giving forth life becoming burden Kenya This m...</td>\n",
       "      <td>0.3194444444444445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teenmaar crPanja crGabbarsingh cr Khaleja Kuda...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rei chintu lo Vachina Ad Nizam ne lo kottaru f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today World Day Combat Restoring degraded land...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearing say 's confident vaccines delivered li...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5616</th>\n",
       "      <td>Lambie voted AGAINST Medivac Carbon tax protec...</td>\n",
       "      <td>0.13636363636363635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>Idhi bathuku Most Day1 Records In Nizam Non BB...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>welkin moon giveaway bc finally money -u got t...</td>\n",
       "      <td>0.15416666666666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>Manam edi chesina Daaniki kuda elevation istad...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>This first time I 've seen Government media le...</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5621 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text             polarity\n",
       "0     Giving forth life becoming burden Kenya This m...   0.3194444444444445\n",
       "1     Teenmaar crPanja crGabbarsingh cr Khaleja Kuda...                  0.0\n",
       "2     Rei chintu lo Vachina Ad Nizam ne lo kottaru f...                  0.0\n",
       "3     Today World Day Combat Restoring degraded land...                 0.25\n",
       "4     Hearing say 's confident vaccines delivered li...                  0.5\n",
       "...                                                 ...                  ...\n",
       "5616  Lambie voted AGAINST Medivac Carbon tax protec...  0.13636363636363635\n",
       "5617  Idhi bathuku Most Day1 Records In Nizam Non BB...                  0.5\n",
       "5618  welkin moon giveaway bc finally money -u got t...  0.15416666666666667\n",
       "5619  Manam edi chesina Daaniki kuda elevation istad...                  0.0\n",
       "5620  This first time I 've seen Government media le...                 0.13\n",
       "\n",
       "[5621 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanTweet = pd.DataFrame(df[['clean_text','polarity']])\n",
    "cleanTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.3194444444444445\n",
       "1                       0.0\n",
       "2                       0.0\n",
       "3                      0.25\n",
       "4                       0.5\n",
       "               ...         \n",
       "5616    0.13636363636363635\n",
       "5617                    0.5\n",
       "5618    0.15416666666666667\n",
       "5619                    0.0\n",
       "5620                   0.13\n",
       "Name: polarity, Length: 5621, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3zfJNdlPDf1Y"
   },
   "outputs": [],
   "source": [
    "def text_category(p):\n",
    "    values = ['positive','negative','neutral']\n",
    "    for value in values:\n",
    "        if p == 1:\n",
    "            value = values[0]\n",
    "        elif p == 2:\n",
    "            value = values[1]\n",
    "        elif p ==3:\n",
    "            value = values[2]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "text_category() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-9b63f431c492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanTweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'polarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_category\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda4\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Anaconda4\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   4183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4184\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4185\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: text_category() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "a = cleanTweet['polarity'].apply(text_category, axis = 0)\n",
    "score = pd.DataFrame(a)\n",
    "score.info(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweet = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJSoVXXPDf1Z"
   },
   "source": [
    "# EXTENSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwLNGJKMDf1Z"
   },
   "source": [
    "### Using Cross-Validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx0cJ7LeDf1Z"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;For this part we will use `RandomizedSearchCV`<sup>(12)</sup> which chooses the parameters randomly from the list that we give, or according to the distribution that we specify from `scipy.stats` (e.g. uniform); then is estimates the test error by doing cross-validation and after all iterations we can find the best estimator, the best parameters and the best score in the variables `best_estimator_`, `best_params_` and `best_score_`.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Because the search space for the parameters that we want to test is very big and it may need a huge number of iterations until it finds the best combination, we will split the set of parameters in 2 and do the hyper-parameter tuning process in two phases. First we will find the optimal combination of loss, learning_rate and eta0 (i.e. initial learning rate); and then for penalty and alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Dazi6Y6Df1Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXAqokTMDf1a"
   },
   "outputs": [],
   "source": [
    "X_train = X_train_bigram_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loLZ_qslDf1a"
   },
   "source": [
    "#### Phase 1: loss, learning rate and initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy2RVuaSDf1a"
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0br7A0lgDf1b"
   },
   "outputs": [],
   "source": [
    "distributions = dict(\n",
    "    loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "    eta0=uniform(loc=1e-7, scale=1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MANfqxiiDf1b"
   },
   "outputs": [],
   "source": [
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGoTN6V5Df1b"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Because we got \"learning_rate = optimal\" to be the best, then we will ignore the eta0 (initial learning rate) as it isn't used when learning_rate='optimal'; we got this value of eta0 just because of the randomness involved in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-rtaC8MDf1c"
   },
   "source": [
    "#### Phase 2: penalty and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKk0MSB2Df1c"
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPYDmyKNDf1d"
   },
   "outputs": [],
   "source": [
    "distributions = dict(\n",
    "    penalty=['l1', 'l2', 'elasticnet'],\n",
    "    alpha=uniform(loc=1e-6, scale=1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njpOXvz-Df1d"
   },
   "outputs": [],
   "source": [
    "random_search_cv = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=distributions,\n",
    "    cv=5,\n",
    "    n_iter=50\n",
    ")\n",
    "random_search_cv.fit(X_train, y_train)\n",
    "print(f'Best params: {random_search_cv.best_params_}')\n",
    "print(f'Best score: {random_search_cv.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azmWqtnWDf1e"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;So, the best parameters that I got are:  \n",
    "`loss: squared_hinge  \n",
    " learning_rate: optimal  \n",
    " penalty: l2  \n",
    " alpha: 1.2101013664295101e-05  `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jilcbh5CDf1e"
   },
   "source": [
    "#### Saving the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_cN2kieDf1e"
   },
   "outputs": [],
   "source": [
    "!mkdir 'classifiers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZYb9dO-Df1f"
   },
   "outputs": [],
   "source": [
    "sgd_classifier = random_search_cv.best_estimator_\n",
    "\n",
    "dump(random_search_cv.best_estimator_, 'classifiers/sgd_classifier.joblib')\n",
    "\n",
    "# sgd_classifier = load('classifiers/sgd_classifier.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxCvyAqaDf1f"
   },
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xg9qhT5eDf1f"
   },
   "outputs": [],
   "source": [
    "X_test = bigram_vectorizer.transform(imdb_test['text'].values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = imdb_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdkCo2NtDf1f"
   },
   "outputs": [],
   "source": [
    "score = sgd_classifier.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exp0TkJYDf1f"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;And we got **90.18%** test accuracy. That's not bad for our simple linear model. There are more advanced methods that give better results. The current state-of-the-art on this dataset is **97.42%** <sup>(13)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtp6Lw53Df1g"
   },
   "source": [
    "## References\n",
    "\n",
    "<sup>(1)</sup> &nbsp;[Sentiment Analysis - Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)  \n",
    "<sup>(2)</sup> &nbsp;[Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)  \n",
    "<sup>(3)</sup> &nbsp;[Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)  \n",
    "<sup>(4)</sup> &nbsp;[Tf-idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)  \n",
    "<sup>(5)</sup> &nbsp;[TfidfTransformer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)  \n",
    "<sup>(6)</sup> &nbsp;[Stop words - Wikipedia](https://en.wikipedia.org/wiki/Stop_words)  \n",
    "<sup>(7)</sup> &nbsp;[A list of English stopwords](https://gist.github.com/sebleier/554280)  \n",
    "<sup>(8)</sup> &nbsp;[CountVectorizer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n",
    "<sup>(9)</sup> &nbsp;[Scipy sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html)  \n",
    "<sup>(10)</sup> [Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)  \n",
    "<sup>(11)</sup> [SGDClassifier - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)  \n",
    "<sup>(12)</sup> [RandomizedSearchCV - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n",
    "<sup>(13)</sup> [Sentiment Classification using Document Embeddings trained with\n",
    "Cosine Similarity](https://www.aclweb.org/anthology/P19-2057.pdf)  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of sentiment-analysis.ipynb",
   "provenance": [
    {
     "file_id": "120eminHIw7SVwyktMBU_ej8-10wF6qqo",
     "timestamp": 1624384286919
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
